{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Overview  \nThis notebook sourcecode is a reproduction of Aurelien Geron's sourcecode for a Spam filter. He is the author of Hands-On Machnine Learning with Scikit-Learn, Keras, & Tensorflow, the book I'm using to study ML. This notebook contains my notes and commentary explaining the sourcecode.\n    The spam filter works by creating a vocabulary of the most commonly occuring words and creates a vector of counts of how many times those words occur in each email. That vector of counts is then fed into a logistic regression model.  \n    \n1. Fetch and Load Data\n2. Preview Data\n3. Split the Data into Train and Test Sets\n4. Feed Data into a Preparation Pipeline\n5. Train the Model\n6. Predict the Test Data and Score"},{"metadata":{},"cell_type":"markdown","source":"# Fetch and Load Data  \nSpecify the local download folders and server url containing the datasets. The download folders are created if they dont exist. The dataset files are checked for existence and downloaded if not. The dataset file is a compressed archive .tar.bz2. The files are extracted and their filenames are loaded into separate lists according to whether they're spam or ham. The contents are then parsed from each file into an email structure.\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"'''\nThe tarfile module allows us to decompress and unpack the dataset. The .tar file is an archive, the name comess from\n\"tape archive\". The .tar file is compressed with Bzip2, hence the .bz2 extension. There are many compression algorithms. Gzip is\nanother common program for compressing files. \n\nsix.moves: Six provides simple utilities for wrapping over differences between Python 2 and Python 3. It is intended to support codebases\nthat work on both Python 2 and 3 without modification. six consists of only one Python file, so it is painless to copy into a project.\n'''\nimport tarfile\nfrom six.moves import urllib\n\nDOWNLOAD_ROOT = \"https://spamassassin.apache.org/old/publiccorpus/\"\nHAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\nSPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\nSPAM_PATH = os.path.join(\"datasets\", \"spam\")  \n# os.path.join joins the paths intelligently according to the OS. SPAM_PATH is the directory where the dataset is held locally","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SPAM_PATH is the local parent directory for our dataset\n\ndef fetch_spam_data(spam_url=SPAM_URL, spam_path=SPAM_PATH):\n    #check if directory exists, create if doesn't\n    if not os.path.isdir(spam_path):\n        os.makedirs(spam_path)\n    #check if dataset files exist. if does not exist, download and extract as \"ham.tar.bz2\" and \"spam.tar.bz2\"\n    for filename, url in ((\"ham.tar.bz2\", HAM_URL), (\"spam.tar.bz2\", SPAM_URL)):\n        #path is the local filename with its path\n        path = os.path.join(spam_path, filename)\n        if not os.path.isfile(path):\n            urllib.request.urlretrieve(url, path)\n        tar_bz2_file = tarfile.open(path)\n        tar_bz2_file.extractall(path=SPAM_PATH)\n        tar_bz2_file.close()\n        \nfetch_spam_data()","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#specify the directories containing spam's and ham's archive contents\nHAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\nSPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\n\n#get file with filename of greater that 20 character length.\nham_filenames = [filename for filename in sorted(os.listdir(HAM_DIR)) if len(filename) > 20]\nspam_filenames = [filename for filename in sorted(os.listdir(SPAM_DIR)) if len(filename) > 20]\n\nprint(\"Sample filenames:\\n\", ham_filenames[:5], \"\\n\", spam_filenames[:5])\nprint(\"There are\", len(spam_filenames) ,\"spam files and\",  len(ham_filenames) ,\"ham files.\\n\")","execution_count":6,"outputs":[{"output_type":"stream","text":"Sample filenames:\n ['00001.7c53336b37003a9286aba55d2945844c', '00002.9c4069e25e1ef370c078db7ee85ff9ac', '00003.860e3c3cee1b42ead714c5c874fe25f7', '00004.864220c5b6930b209cc287c361c99af1', '00005.bf27cdeaf0b8c4647ecd61b1d09da613'] \n ['00001.7848dde101aa985090474a91ec93fcf0', '00002.d94f1b97e48ed3b553b3508d116e6a09', '00003.2ee33bc6eacdb11f38d052c44819ba6c', '00004.eac8de8d759b7e74154f142194282724', '00005.57696a39d7d84318ce497886896bf90d']\nThere are 500 spam files and 2500 ham files.\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Load the Data"},{"metadata":{},"cell_type":"markdown","source":"Use Python's **email** module to parse email files. According to documentation, the **email policy** should be specified when creating a BytesParse instance. The 'email policy' seems to specify the structure of the email so that the parsing method maps the file contents to some certain object properties.  \n\nCustom functions to load emails and parse emails are defined. Since spam and ham emails are in different folder, one of the parameters has to specify\nwhich folder to look into. The two other parameters the function takes is a specific filename and the parent folder of \"/spam/\" and \"/easy_ham\".  \n\nThe spam_emails and ham_emails consists of EmailMessage objects returned by the load_email function. Below are two of EmailMessage's methods.\n* get_content()  \nreturns a string of the Message minus the header\n* get_payload()  \nReturn the current payload, which will be a list of Message objects when is_multipart() is True, or a string when is_multipart() is False. If the payload is a list and you mutate the list object, you modify the message’s payload in place.\n* get_content_type()  \nReturn the message’s content type. The returned string is coerced to lower case of the form maintype/subtype. If there was no Content-Type header in the message the default type as given by get_default_type() will be returned. Since according to RFC 2045, messages always have a default type, get_content_type() will always return a value."},{"metadata":{"trusted":true},"cell_type":"code","source":"import email\nimport email.policy\n\ndef load_email(is_spam, filename, parent_dir=SPAM_PATH): #the last parameter is defaulted SPAM_PATH\n    #determine which folder to look in, spam or easy_ham\n    folder = \"spam\" if is_spam else \"easy_ham\"\n    #open the file specified by filename, using 'with' to free resources after returning. flag open() to read-only and binary modes\n    with open(os.path.join(parent_dir, folder, filename), 'rb') as f:\n        #specify email policy\n        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n    \n#load emails\nspam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]\nham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\n        ","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preview Data  \nLook at a sample email in its entirety and another sample email's contents stripped of leading and trailing whitespace characters. Some emails are multipart, with images and attachments (which can have their own attachments). We write a recursive function to get the email structure. "},{"metadata":{"trusted":true},"cell_type":"code","source":"display(type(ham_emails[1]))\nprint(ham_emails[1])","execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"email.message.EmailMessage"},"metadata":{}},{"output_type":"stream","text":"Return-Path: <Steve_Burt@cursor-system.com>\nDelivered-To: zzzz@localhost.netnoteinc.com\nReceived: from localhost (localhost [127.0.0.1])\n\tby phobos.labs.netnoteinc.com (Postfix) with ESMTP id BE12E43C34\n\tfor <zzzz@localhost>; Thu, 22 Aug 2002 07:46:38 -0400 (EDT)\nReceived: from phobos [127.0.0.1]\n\tby localhost with IMAP (fetchmail-5.9.0)\n\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 12:46:38 +0100 (IST)\nReceived: from n20.grp.scd.yahoo.com (n20.grp.scd.yahoo.com    [66.218.66.76])\n by dogma.slashnull.org (8.11.6/8.11.6) with SMTP id    g7MBkTZ05087 for\n <zzzz@spamassassin.taint.org>; Thu, 22 Aug 2002 12:46:29 +0100\nX-Egroups-Return: =?utf-8?q?sentto-2242572-52726-1030016790-zzzz=3Dspamassas?=\n =?utf-8?q?sin=2Etaint=2Eorg=40returns=2Egroups=2Eyahoo=2Ecom?=\nReceived: from [66.218.67.196] by n20.grp.scd.yahoo.com with NNFMP;\n    22 Aug 2002 11:46:30 -0000\nX-Sender: steve.burt@cursor-system.com\nX-Apparently-To: zzzzteana@yahoogroups.com\nReceived: (EGP: mail-8_1_0_1); 22 Aug 2002 11:46:29 -0000\nReceived: (qmail 11764 invoked from network); 22 Aug 2002 11:46:29 -0000\nReceived: from unknown (66.218.66.217) by m3.grp.scd.yahoo.com with QMQP;\n    22 Aug 2002 11:46:29 -0000\nReceived: from unknown (HELO mailgateway.cursor-system.com) (62.189.7.27)\n    by mta2.grp.scd.yahoo.com with SMTP; 22 Aug 2002 11:46:29 -0000\nReceived: from exchange1.cps.local (unverified) by\n    mailgateway.cursor-system.com (Content Technologies SMTPRS 4.2.10) with\n    ESMTP id <T5cde81f695ac1d100407d@mailgateway.cursor-system.com> for\n    <forteana@yahoogroups.com>; Thu, 22 Aug 2002 13:14:10 +0100\nReceived: by exchange1.cps.local with Internet Mail Service (5.5.2653.19)\n    id <PXX6AT23>; Thu, 22 Aug 2002 12:46:27 +0100\nMessage-Id: <5EC2AD6D2314D14FB64BDA287D25D9EF12B4F6@exchange1.cps.local>\nTo: \"'zzzzteana@yahoogroups.com'\" <zzzzteana@yahoogroups.com>\nX-Mailer: Internet Mail Service (5.5.2653.19)\nX-Egroups-From: Steve Burt <steve.burt@cursor-system.com>\nFrom: Steve Burt <Steve_Burt@cursor-system.com>\nX-Yahoo-Profile: pyruse\nMIME-Version: 1.0\nMailing-List: list zzzzteana@yahoogroups.com; contact\n    forteana-owner@yahoogroups.com\nDelivered-To: mailing list zzzzteana@yahoogroups.com\nPrecedence: bulk\nList-Unsubscribe: <mailto:zzzzteana-unsubscribe@yahoogroups.com>\nDate: Thu, 22 Aug 2002 12:46:18 +0100\nSubject: [zzzzteana] RE: Alexander\nReply-To: zzzzteana@yahoogroups.com\nContent-Type: text/plain; charset=US-ASCII\nContent-Transfer-Encoding: 7bit\n\nMartin A posted:\nTassos Papadopoulos, the Greek sculptor behind the plan, judged that the\n limestone of Mount Kerdylio, 70 miles east of Salonika and not far from the\n Mount Athos monastic community, was ideal for the patriotic sculpture. \n \n As well as Alexander's granite features, 240 ft high and 170 ft wide, a\n museum, a restored amphitheatre and car park for admiring crowds are\nplanned\n---------------------\nSo is this mountain limestone or granite?\nIf it's limestone, it'll weather pretty fast.\n\n------------------------ Yahoo! Groups Sponsor ---------------------~-->\n4 DVDs Free +s&p Join Now\nhttp://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\n---------------------------------------------------------------------~->\n\nTo unsubscribe from this group, send an email to:\nforteana-unsubscribe@egroups.com\n\n \n\nYour use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/ \n\n\n\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(spam_emails[50].get_content().strip())","execution_count":9,"outputs":[{"output_type":"stream","text":"Help wanted.  We are a 14 year old fortune 500 company, that is\ngrowing at a tremendous rate.  We are looking for individuals who\nwant to work from home.\n\nThis is an opportunity to make an excellent income.  No experience\nis required.  We will train you.\n\nSo if you are looking to be employed from home with a career that has\nvast opportunities, then go:\n\nhttp://www.basetel.com/wealthnow\n\nWe are looking for energetic and self motivated people.  If that is you\nthan click on the link and fill out the form, and one of our\nemployement specialist will contact you.\n\nTo be removed from our link simple go to:\n\nhttp://www.basetel.com/remove.html\n\n\n4592gPjt0-916msGW0934HwlS5-965Tqzv4189Rjvx0-174yaja0756SEjNl56\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Inspect Email Structure  \nThe function handles a string, an EmailMessage object, or list of EmailMessage objects. A string argument results in that string being returned. An EmailMessage argument results in the EmailMessage's content type being returned. A list will lead to recursion. "},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nisinstance(object, classinfo)\nReturn True if the object argument is an instance of the classinfo argument, or of a (direct, indirect or virtual) subclass thereof. \nIf object is not an object of the given type, the function always returns False. If classinfo is a tuple of type objects (or recursively, other such tuples), \nreturn True if object is an instance of any of the types. If classinfo is not a type or tuple of types and such tuples, a TypeError exception is raised.\n'''\n\ndef get_email_structure(email):\n    #check if 'email' is a string. \n    if isinstance(email, str):\n        return email\n    #otherwise get the message payload\n    content = email.get_payload()\n    if isinstance(content, list):\n        return \"multipart({})\".format(\",\".join([get_email_structure(sub_email) for sub_email in content]))\n    else:\n        return email.get_content_type()\n    \n#example email from the dataset\nget_email_structure(spam_emails[91])","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"'multipart(text/html)'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nA Counter is a dict subclass for counting hashable objects. It is a collection where elements are stored as dictionary keys and their counts are stored as \ndictionary values. Counts are allowed to be any integer value including zero or negative counts. The Counter class is similar to bags or multisets in other \nlanguages\n'''\nfrom collections import Counter\n\ndef structures_counter(emails):\n    structures = Counter()\n    for email in emails:\n        structure = get_email_structure(email)\n        structures[structure] += 1\n    return structures","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"structures_counter(spam_emails).most_common()","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"[('text/plain', 218),\n ('text/html', 183),\n ('multipart(text/plain,text/html)', 45),\n ('multipart(text/html)', 20),\n ('multipart(text/plain)', 19),\n ('multipart(multipart(text/html))', 5),\n ('multipart(text/plain,image/jpeg)', 3),\n ('multipart(text/html,application/octet-stream)', 2),\n ('multipart(text/plain,application/octet-stream)', 1),\n ('multipart(text/html,text/plain)', 1),\n ('multipart(multipart(text/html),application/octet-stream,image/jpeg)', 1),\n ('multipart(multipart(text/plain,text/html),image/gif)', 1),\n ('multipart/alternative', 1)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Split Data into Train and Test Sets  \nMerge the spam and ham email into a single numpy array. Create labels by constructing a vector of 0's and 1's."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = np.array(ham_emails + spam_emails)\ny = np.array([0] * len(spam_emails) + [1] * len(ham_emails))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n#specifying a \"random state\" seeds the random number generator with the same number for each run so that we'll get the same results each time the script is ran\n","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build a Preprocessing Pipeline  \nThe preprocessing pipeline consists of two main parts, one counts the words in each email and the other turns the word counts into vectors. The following processes are part of the pipeline and are turned on/off by parameters flags, which are set to True by default.  \n1. Strip Headers\n2. Convert to Lowercase\n3. Remove Punctuations\n4. Replace URLs\n5. Replace Numbers\n6. Stem words (e.g. running -> run)  \n\nThe pipeline requires support functions, a email-to-text function which uses an html-to-text function, that we'll write before constructing the pipeline."},{"metadata":{},"cell_type":"markdown","source":"## Support Functions for Pipeline  "},{"metadata":{},"cell_type":"markdown","source":"### HTML to Text and Email to Text  \nThis function gets rid of HTML tags, whitespace characters, newline characters, and unescapes html code into its characters (e.g. &lt; -> '<'). Hyperlinks tags are replaced with ' HYPERLINK '."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re  #regular expression package\nfrom html import unescape \n\ndef html_to_text(html):\n    text = re.sub('<head.*?>.*?</head>', '', html, flags= re.M | re.S | re.I) \n    text = re.sub('<a.*?>.*?</a>', ' HYPERLINK ', text, flags= re.M | re.S | re.I)\n    text = re.sub('<.*?>', '', text,  flags= re.M | re.S | re.I)\n    text = re.sub(r'(\\s*\\n)+', '\\n', text,  flags= re.M | re.S | re.I)\n    return unescape(text)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nExamples of get_content_type() return values\n         'multipart/signed': 68,\n         'multipart/alternative': 9,\n         'multipart/mixed': 10,\n         'multipart/related': 3,\n         'multipart/report': 2\n'''\n\ndef email_to_text(email):\n    html = None\n    for part in email.walk():\n        ctype = part.get_content_type()\n        if not ctype in (\"text/plain\", \"text/html\"):\n            continue\n        try:\n            #this part will result in a KeyError if 'part' is multipart \n            content = part.get_content()    \n        except:\n            content = str(part.get_payload())     \n            #print(content)\n        if ctype == 'text/plain':\n            return content\n        else:\n            html = content\n        \n    if html:\n        return html_to_text(html)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"html_spam_email = [email for email in X_train[y_train == 1] if get_email_structure(email) == \"text/html\"]\nsample_html_spam = html_spam_email[7]\nprint(html_to_text(sample_html_spam.get_content())[:1000], \"...\")","execution_count":16,"outputs":[{"output_type":"stream","text":"\nOTC\n Newsletter\nDiscover Tomorrow's Winners \nFor Immediate Release\nCal-Bay (Stock Symbol: CBYI)\nWatch for analyst \"Strong Buy Recommendations\" and several advisory newsletters picking CBYI.  CBYI has filed to be traded on the OTCBB, share prices historically INCREASE when companies get listed on this larger trading exchange. CBYI is trading around 25 cents and should skyrocket to $2.66 - $3.25 a share in the near future.\nPut CBYI on your watch list, acquire a position TODAY.\nREASONS TO INVEST IN CBYI\nA profitable company and is on track to beat ALL earnings estimates!\nOne of the FASTEST growing distributors in environmental & safety equipment instruments.\nExcellent management team, several EXCLUSIVE contracts.  IMPRESSIVE client list including the U.S. Air Force, Anheuser-Busch, Chevron Refining and Mitsubishi Heavy Industries, GE-Energy & Environmental Research.\nRAPIDLY GROWING INDUSTRY\nIndustry revenues exceed $900 million, estimates indicate that there could be as much as $25 billi ...\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Porter Stemmer and URL Extractor"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    import nltk\n    \n    stemmer = nltk.PorterStemmer()\nexcept ImportError:\n    print(\"Error: Stemming requires the NLTK module.\")\n    stemmer = None","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    !pip install -q -U urlextract #issue a command to operating system\nexcept ImportError:\n    print(\"Couldn't install urlextract\")\n","execution_count":18,"outputs":[{"output_type":"stream","text":"\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\r\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    import urlextract\n    url_extractor = urlextract.URLExtract()\nexcept ImportError:\n    print(\"Error: replacing URLs requires the urlextract module.\")\n    url_extractor = None\n        ","execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Constructing the Pipeline  \nConstruct the two transformers to be used to in the pipeline then put the pipeline together."},{"metadata":{},"cell_type":"markdown","source":"### Email to Word Counter  \nIterate through emails passed to the EmailToWordCounterTransformer. Convert each email into text. Process the text according to flags. Count the stemmed words. Put the counts into an array."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import TransformerMixin, BaseEstimator\n\nclass EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, strip_header=True, lower_case=True, remove_punctuation=True, replace_urls=True, replace_numbers=True, stemming=True):\n        self.strip_header=strip_header\n        self.lower_case=lower_case\n        self.remove_punctuation=remove_punctuation\n        self.replace_urls=replace_urls\n        self.replace_numbers=replace_numbers\n        self.stemming=stemming\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X_transformed = [] #list to store word counts for each email\n        for email in X:\n            #convert email to text\n            text = email_to_text(email) or \"\" \n            #process text accoring to flags. some flags aren't used :(\n            if self.lower_case:\n                text = text.lower()\n            if self.replace_urls and url_extractor is not None:\n                #create a list of URLs in the email and sort them according to length. To eliminate redundant URLs, we turn them into a set first.\n                urls = list(set(url_extractor.find_urls(text)))\n                urls.sort(key=lambda url: len(url), reverse=True)\n                for url in urls:\n                    text = text.replace(url, ' URL ')\n            if self.replace_numbers:\n                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))', ' NUMBER ', text) #replaces scientific formatted numbers\n            if self.remove_punctuation:\n                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n            \n            #create a Counter object initialized with the emails words\n            word_counts = Counter(text.split())\n            if self.stemming and stemmer is not None:\n                stemmed_word_counts = Counter()\n                for word, count in word_counts.items():\n                    stemmed_word = stemmer.stem(word)\n                    stemmed_word_counts[stemmed_word] += count\n                word_counts = stemmed_word_counts\n            X_transformed.append(word_counts)\n        return np.array(X_transformed)","execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word Counter to Vector Transformer  \nFor each email, create a vector of counts of the n-most occuring words. The n most occuring words are found by sorting and slicing a dictionary of all words occuring in all emails. We construct and return a sparse matrix of the counts. Each row corresponds to an email, each column corresponds to the occurences of a vocubalry word in that email. The 0th element is the non-vocabulary words.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import csr_matrix\n\nclass WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, vocab_size=1000):\n        self.vocab_size=vocab_size\n    def fit(self, X, y=None):\n        total_count = Counter()\n        for word_count in X: #X is a list of Counters\n            for word, count in word_count.items():\n                total_count[word] += min(count, 10) #cap the increment to 10\n        most_common = total_count.most_common()[:self.vocab_size] #get the n most common words\n        \n        #create instance variables for inspection\n        self.most_common_ = most_common\n        #self.vocabulary_ is offset by 1 for assisting in creating the a matrix. The values of these key:value pairs\n        #will correspond to the column indices in the upcoming matrix produced in transform()\n        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n        return self\n    def transform(self, X, y=None):\n        rows = []\n        cols = []\n        data = []\n        for row, word_count in enumerate(X):\n            for word, counts in word_count.items():\n                rows.append(row)\n                cols.append(self.vocabulary_.get(word, 0))\n                data.append(counts)\n        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocab_size+1))\n        ","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_few = X_train[:3]\nX_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n\nvocab_transformer = WordCounterToVectorTransformer(vocab_size=10)\nX_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)","execution_count":22,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Construct the Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n\npreprocessing_pipeline = Pipeline([\n    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n    (\"wordcount_to_vector\", WordCounterToVectorTransformer())\n])","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_transformed = preprocessing_pipeline.fit_transform(X_train)","execution_count":28,"outputs":[{"output_type":"stream","text":"<html><xbody>\n<hr width = \"100%\">\n<center><font size = \"+1\" color =\n\"blue\"><b>Over $100,000 The First Year, Most Of That While I Was Sleeping!  Will Work For Anyone, Anywhere!</font></b><p>\n<table><Tr><td>\n\n      <p align=\"center\"><font face=\"Verdana, Arial, Helvetica, sans-serif\" size=\"2\"><b><font color=\"#000000\" size=\"4\" face=\"Arial\">Imagine \n        The Perfect Business</font></b></font></p>\n      <ul>\n        <li><font face=\"Verdana, Arial, Helvetica, sans-serif\" size=\"2\"><b>You \n          Can Run It From Home...Or From Anywhere With A Telephone Connection</b></font></li>\n      </ul>\n      <ul>\n        <li><font face=\"Verdana, Arial, Helvetica, sans-serif\" size=\"2\" color=\"#000099\"><b><font color=\"#000000\">There \n          Is No Large Investment To Get Started</font></b></font></li>\n      </ul>\n      <ul>\n        <li><font face=\"Verdana, Arial, Helvetica, sans-serif\" size=\"2\"><b>You \n          Can Put Everything On Auto-Pilot</b></font></li>\n      </ul>\n      <ul>\n        <li><font face=\"Verdana, Arial, Helvetica, sans-serif\" size=\"2\"><b><font color=\"#000000\">There \n          Is No Face-To-Face Selling Involved</font></b></font></li>\n      </ul>\n      <ul>\n        <li><font face=\"Verdana, Arial, Helvetica, sans-serif\" size=\"2\"><b>There \n          Is No Inventory To Buy Or Ship</b></font></li>\n      </ul>\n      <ul>\n        <li><font color=\"#000099\" face=\"Verdana, Arial, Helvetica, sans-serif\" size=\"2\"><b><font color=\"#000000\">You \n          Can Even Work In Your Underwear If You Want To (No More Ties!)</font></b></font></li>\n      </ul>\n      <ul>\n        <li><font color=\"#000099\" face=\"Verdana, Arial, Helvetica, sans-serif\" size=\"2\"><b><font color=\"#000000\">If You Are Willing To Put 1 Or 2 Hours In To Start, This Can Be A Mint!  <font color=\"red\">If You Can't Spare A Few Hours & The Money For A Movie For Two & A Snack, Don't Go Any Further!</font></b></font></li>\n      </ul>\n\n\n      <p><font face=\"Verdana, Arial, Helvetica, sans-serif\" size=\"2\">Sound good? \n        It Is! </font></p>\n      <p align=\"center\"><font face=\"Verdana, Arial, Helvetica, sans-serif\" size=\"2\"><tt><font color=\"#CC0000\"><b><font color=\"#000000\" size=\"4\" face=\"Arial\">The \n        Cash Flow System That Brought In Over $115,467.00 In The First Year</font></b></font></tt></font></p>\n      <p align=\"left\"><font face=\"Verdana, Arial, Helvetica, sans-serif\" size=\"2\"><b><font color=\"blue\">My \n        Internet Business brought in over $115,467.21 last year and I'm going \n        to show you exactly how you can do the same thing. But I'm not going \n        to stop there...I'm going to give you FIVE Internet businesses you can \n        start right now...TODAY!</font></b></font></p>\n\n</td></tr></table>\n<br><font size = \"+1\">Please Visit Our\nWebsite<p></font>\nAnd Place Your <font color = \"red\"> <b>Order\nTODAY!</b> </font><a target=\"_blank\"  href =\n\"http://www.geocities.com/instant_net2002/\"><b><font\nsize=\"5\">CLICK HERE</font></b> </a><p>&nbsp;<p>\n</center>\nTo Order by postal mail, please send to the below address.  Or order via fax from our\n24 hour fax line:  734-574-6455\n<p>\nMake payable to Instant Internet Empire.\n<br><br>\nInstant Internet Empire<br>\n238 East Southern Ave.<br>\nSpringfield, Ohio  45505<br>\n<br>\n___Instant Internet Empire $42.95<br>\nAdd $3.00 Processing Fee To Your Order.\n<br><br>\n*****<br>\n<b><font color=\"red\">Important Credit Card Information! Please Read Below!</b></font>\n <br><br>\n*     Credit Card Address, City, State and Zip Code, must match\n      billing address to be processed. \n<br><br>\n\nCHECK____  MONEYORDER____  VISA____ MASTERCARD____ AmericanExpress___\nDebt Card___\n<br><br>\nName_______________________________________________________<br>\n(As it appears on Check or Credit Card)\n<br><br>\nAddress____________________________________________________<br>\n(As it appears on Check or Credit Card)\n<br><br>\n___________________________________________________<br>\nCity,State,Zip(As it appears on Check or Credit Card)\n<br><br>\n___________________________________________________<br>\nCountry\n<br><br>\n___________________________________________________<br>\n(Credit Card Number)\n<br><br>\nExpiration Month_____  Year_____\n<br><br>\n______________________________________________<br>\nPhone Number\n<br><br>\n___________________________________________________________<br>\nEmail Address:  All Information Sent Via Email Address (Please Write Neat)!\n<br><br>\n___________________________________________________________<br>\nAuthorized Signature\n<p>\n<b>\nTo Be Removed From Our Mailing List, Simply Put The Word \"Remove\" In The Subject Line, And Send To The Email Address Below.  <font color=\"red\">For Those Who Wish To Be Removed, Please Don't Complain To The Remove Name ISP.  A Remove List Can't Be Make, If You Have It Shut Down!</font></b>\n<p>\nEmail Address:  <a href=\"mailto:takemeoff2002@hotpop.com\">REMOVE</a>\n\n</BODY>\n</HTML>\n\n\n\n                         Foreword\n\nAfter thirty?three years of being a black man living in \"Ghetto America,\" an environment notorious for danger, crime, and poverty, I have made startling discoveries involving young African?American men and women who comprise what I have come to define as the \"Black Generation X\". I have become aware that there is no formal distinction separating young African?Americans from the general definition of \"Generation X\". Aside from it being regarded as the generation that followed \"Baby Boomers\", it's important to point out there is no root definition for this term as well. Since the theme of this work is centered on identifying the realities and misconceptions of young African?American culture, I have chosen a definition that incorporates all of the components that were part of my birth, childhood, and young adult life. Taken from a web site entitled \"Generation X\", created and maintained by the Colorado College, the definition is as follows: \n \n \"We are a group of people born between 1961 and 1981. We are individuals who live for the 'here and now', like to experiment, and who require immediate results. We are typically selfish, and cynical, and depend a lot on our parents. We question authority, and feel like we carry the burden of the previous generations. It's that simple. It seems we have come to be called \"Xer's\" simply because we represent something negative to our elders. We may be the one thing all of the generations that precede us have in common, that is the ability to speak assuredly about our shortcomings. Of course, they overlook the fact that we are their responsibility, or actually their fault. Our generation will be called upon to look after our parents knowing they failed to look after us. Intergenerational justice failed somewhere along the way and it will be our task to either rectify it or make it worse.\"\n\"We are a group of individuals who grew up with no one at home after school. It appears we have little hope for the future. No jobs, no homes, and basically no money are almost expected of us. These bleak prospects, along with the fact that we will be forced to support the largest amount of senior citizens ever, do not provide much hope. Some believe that these blockades will be too much for us to handle and we will for the most part fail at life, but many see our individualism and resourcefulness that has been built up through our childhood, as our saviors. We will soon discover who is right and who is wrong.\" \n\nThis definition is important because it demonstrates the widely held belief that Generation X is comprised of one core group of American citizens. It over generalizes the similarities it expresses, and applies them to all those who were born within the corresponding years. When in fact these \"similarities\" speak primarily to only \"white\" issues and concerns. For instance, \"they\" say \"we\" depend a lot on our parents, but in ghetto America \"we\" could only depend on one �our mothers! \"They\" also say, \" . . .we feel like we carry the burden of previous generations,\" and \" . . . we will be forced to support the largest amount of senior citizens ever.\" In ghetto America, no one ever gives these issues any thought because survival is in the here and now, and is all \"we\" have time for. \nI have known for many years that the differences that separate Generation Xer's black and white occupants would eventually be identified. I never knew I would be the one doing it. Extracting the \"Black\" from \"Generation X\" and categorizing it for its own sake is a necessary and important contingency. Without doing so, the social ills that plague Ghetto America would be lost within the generalities that are explained away as part of a collective American identity.\nThere are many hidden truths that exist in black America today. \"White America\" charges that we spend our time collecting welfare, refusing to vote, reading elementary styled books, watching music videos, day dreaming of becoming rappers, basketball players, selling drugs, and making babies. Much of this is supported in Marlon Riggs's documentary entitled \"Ethnic Notions.\" When African Americans are mentioned, whites typically envision an image that is strikingly similar to the one that graces the cover of this book. A reality that I confirmed while watching \"Roger & Me,\" another documentary written, and directed by Michael Moore. Sadly, many of us have begun to validate this skewed image, while others bend over backwards in an attempt to pretend we do nothing of the kind, only to be contradicted and embarrassed when presented with the daily programming line up for B.E.T. (Black Entertainment Television). Many of us fail to realize, that whatever embarrassment or shame the ex\n posed truth may reveal, should be taken as an opportunity to improve, empower, enlighten, and ultimately change anyone who is guilty of perpetrating this kind of behavior.\nRegarding Black Women: I have known many in my life and although each has had unique attitudes and personalities, I began to notice the majority of them shared the same collective mentality toward black men as a whole-that black men owed them something, and are required to provide and fulfill what was missing from their lives, be it emotional, physical, or economical.\n Normally that type of assertion would not warrant opposition from me, so long as a reciprocal attitude was extended. In most cases, it has not been. It's funny though, looking back, I am surprised I never paid any attention to the beliefs and attitudes that different black women subscribe to, or the various schools of thought they possessed. Many have placed strict demands and presumptions on the African?American male, some fair and some unfair. Much of what the public believes and understands about the black female's social structure is often filtered by others in the black community who believe these inaccurate and frequently positive representations will somehow change the reality that is. \nWhat originally sparked my attention was an observation I stumbled upon while skimming through and reading books related to black male and female interaction. I noticed virtually all of the books I read in the genre depict an inaccurate and misleading concentration on the affluent African?American perspective. These stories and commentaries tend to suggest that all African?Americans thrive on a middle?to?upper class professional level. They relay stories of relationship problems with \"Huxtable?like\" bachelors and bachelorettes whose only concern is why the other has not confessed his or her love. As I read these stories I couldn't help but become irritated by the blatant omission of the prevailing majority of blue collar or \"working class\" African?Americans. Although there are those in the black community that can identify with the \"Huxtable?like\" dynamic, unfortunately, they are few and far between. \nThe majority of stories I have read seem to be geared toward fantasizing and romanticizing the oppressive state of affairs that have plagued Black America. The black authors who have written these books fear the potential backlash awaiting anyone who would dare air out our \"dirty laundry\". The fact is, truth should never be stifled by fear of any kind. \nI recently had a candid conversation with some very dear female friends who stated that although they fully acknowledge the terrible state of affairs many of our women are currently drowning in, they felt that publicizing their predicament would be viewed as a form of betrayal. As our discussion intensified, I reminded them that the only way change can be effected is by increasing public awareness that change is needed, and oftentimes, not exposing the truth can be more crippling than the truth itself. \nThe purpose of my book is in essence an attempt to change the self?destructive thinking and self?imprisoning behavior that \"black male and female generation X\" have been subscribing to. While discussing this with my friend, she suggested I simplify my writing style so my potential black readers could better understand what I'm attempting to convey. Acknowledging the real possibility of my message being misunderstood, I suggest to those readers that if they want to fully appreciate the finer points of this narrative-to grab a dictionary. (Don't feel bad-there were several times I had to as well!)\nRegarding Black Men: An overwhelming number of us have become preoccupied with being cool, buying sports wear, driving expensive cars, wearing gold or platinum jewelry, and often without legitimate personal wealth and while living under the most deplorable conditions! I have chosen to address this rarely acknowledged side of African?American culture because I have lived it. I have seen the chaos and futility that exists on this level. It is a world of ignorance, selfishness, pre?occupation, and idiocy.\nIn my world there are people who applaud a lack of education, who brag about having served time in U.S. penitentiaries, the types of guns they carry, dope peddling, and chasing women. They champion the notion of having children without any regard to the poor environment many of them currently inhabit. In this world, wearing expensive clothes and carrying fancy handbags is mandatory even if it means spending their last dollar in order to look like they've got a million. It's a world where children dress in over?priced sports wear, mimicking and idolizing whatever popular athlete or rapper is currently in the public eye. \nThinking, such as the belief that fancy clothes are what define one's self?worth, and the possible corruption this thinking breeds, seems to not be an important concern for black mothers. It may come as a shock to know that most African?Americans feel that those who don't have the same, if not better, material possessions, are lesser beings, referring to them as being corny, lame, or broke. Even our \"male rapper\" entertainers promote backward priorities-nothing else matters as long as you look good. \"The Big Tymers\", a popular rap duo, have a song called \"I'm Still Fly\", shown in heavy rotation on B.E.T. and played regularly on black radio stations throughout the country. (\"Fly\" is ghetto vernacular for well dressed, also used to describe a female's beauty). Its signature verse asserts:\n\n\"I got gator boots with a pimped out Gucci\nSuit, can't pay my rent cause all my money's\nspent, but that's O.K. cause I'm still fly.\"\n\nWorse yet, in the song \"Grinding\", the popular duo \"Eclipse\" glorifies the selling of illegal drugs in inner city neighborhoods as children play and dance throughout the entire video. (The term \"grinding\" is ghetto vernacular for the selling of illegal drugs) The fact that this song and video is also shown in heavy rotation on B.E.T. is further evidence that our world is in utter turmoil. Some of the key lyrics to this song are \"grinding�you know what I keep in the lining\" (the area in their coats where they hide illegal drugs) and\n\n\"Patty cake Patty cake I'm the bakers man\nI bake them cakes as fast as I can\nand you can tell cause of how my bread stack up\nand I disguise it as rap so the feds backup\"\n\nIn this verse, the group refers to the process of cooking cocaine and its transformation into cocaine base or crack. When created it is compressed into the form of a large cookie with the average diameter being the size of a typical cake. (hence the term \"cake\" or sometimes \"pie\") Unfortunately for the rap genre, this song seems to strengthen the long standing argument posed by most whites that rap music is nothing more than an entertainment medium catering to violent drug offenders, who want and do nothing more than carry guns and break the law. \nThe negative references I've identified have become an integral part of the urban African?American existence. This mutated reality is ridiculous and unacceptable. This ideology should not be permitted to continue any longer than it already has. There is an overwhelming part of the African?American community that is aware of this crippling mentality. There are also many that are unaware and unfortunately, more that do not even care. Those of us who are in the know have a responsibility to educate and enlighten those of us who are not aware of the negative behavior being perpetrated daily. Whether because of habit or misguided values, this type of ideology is cyclical and thrives off of ignorance. It should be apparent to us all that we cannot wait for some other race to come and save us. We must save ourselves! I am well aware that many of you are probably furious by now, and are thinking, what right do I have to express the opinions I have thus far, as well as those I have al\n luded too. \nI have chosen to chronicle my experiences because of the diversity and seriousness of their content. There is far too much negativity thriving within our culture. There must be acknowledgment and accountability for the truth, whether good or bad. I am aware that not all of \"us\" have succumbed to the crippling mentality that is rampant in our community, and I have chosen to concentrate on individuals who have had a profound impact on my life. Any similarities and generalizations I illustrate are directed solely toward individuals in the black community who share the collective negative thinking and behavior that will hereafter be identified. \nAdditionally, throughout this book I will refer to African?Americans in four different ways; the first being \"African?Americans\", denoting our status as citizens in the United States; secondly, as \"Black\", a reference that identifies and defines our differences and struggles in this country; third, as \"Niggas,\" a term which is often misunderstood and should not be confused with \"Nigger\". The term \"nigga\" has become an accepted form of reference among young black males used to express friendship (hence the term \"My Nigga\") as well as a blanket reference when speaking to, of, or about other black males in a non?personal manner. I felt it necessary to make this distinction because many of the experiences I will refer to mandate the use of the term to effectively communicate the attitude and mood that existed during the time each story took place; and fourth, as \"Ghetto\", this term is largely intended as an internal reference commonly understood by blacks as a means to define the\n  less polished members of our community.  It is important to note that the use of either term has no negative connotation and is therefore not to be associated with any feeling as such. \nMy commentary is based on real?life situations, both good and bad, and how one African?American man was affected by them. To that end, I feel it is necessary to acknowledge that my intention is not to condemn the individuals I have encountered. Despite the negativity they have displayed, I only want to make them aware that the selfish and self debasing attitudes they subscribed to, in many ways, has caused them to become their own worst enemy.\nIn the case of the black female's negative and limited perspective toward black men, the world, and subconsciously themselves, I submit that this mentality is perhaps single?handedly responsible for the lack of success, happiness, and quality of life that is at the center of every African?American woman's fantasy quest. It is also important to note that the selfish and negative attitudes at the core of the tension that exists between black women and men, has been deliberately created by \"White America\". For example, much of the existing resentment between black women and men can be traced as far back as slavery on up to present day. Many past issues have taken on new identities and have re?emerged in the form of corporate acceptance of black women and the corporate exclusion of black men. The resulting economic independence that some black women now enjoy has caused them to challenge and judge the black man for his absence within corporate America, without realizing and ackno\n wledging that this absence is by design. \nThere are other \"core\" issues that have been passed down and nurtured from generation to generation, such as color barriers, light skin versus dark skin, good hair versus bad hair, loyalty, devotion, and economic status. Believe it or not, there was once a time when black women refused to date a black man if he didn't sell drugs or was not \"thugged out\". (a term for being rough or hoodlum-like) The reasoning behind it being that the \"thug,\" and \"drug dealer\" lifestyle was synonymous with financial independence. \nThese issues, among others, relate directly to the confused mentality that exists within our internal society. Sadly that \"confused mentality\" is responsible for much of the neglect the African American \"ghetto\" community receives at the hands of our more affluent brethren. Who have locked it away in the basement, and seems to be more concerned with convincing the rest of the world that \"ghetto America does not exist. Furthermore, the lack of acknowledgement of which I speak should not be confused with the impoverished masses of the African American community.  Their plight is a well-known reality, but in many cases much of ghetto America falls under that same umbrella. Be advised: this book is not a colorful or lighthearted look into \"Black America\".  Rather it should be viewed as a legitimate tool for identifying and changing the backward thinking that so many young African Americans (including myself) have come to perpetuate. Moreover, that \"backward thinking\" is the main \n cause of our inability to strive for a common goal. An outcome that must be met; and can only be met, with Godly faith, positive strategy, cooperation, love, loyalty, self and mutual respect.\n*************************************************************\n To read More go to www.blackrealitypublishing.com\n\n\nTo Unsubscribe: send mail to majordomo@FreeBSD.org\nwith \"unsubscribe freebsd-ports\" in the body of the message\n\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Feed into a Logitistic Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nlog_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\nscore = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\nprint(score, '\\n', score.mean())","execution_count":30,"outputs":[{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n","name":"stderr"},{"output_type":"stream","text":"[CV]  ................................................................\n[CV] .................................... , score=0.892, total=   0.2s\n[CV]  ................................................................\n[CV] .................................... , score=0.879, total=   0.2s\n[CV]  ................................................................\n[CV] .................................... , score=0.904, total=   0.2s\n[0.8925  0.87875 0.90375] \n 0.8916666666666666\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.5s finished\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\n\nX_test_transformed = preprocessing_pipeline.transform(X_test)\n\nlog_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\nlog_clf.fit(X_train_preprocessed, y_train)\n\ny_pred = log_clf.predict(X_test_transformed)\n\nprint(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\nprint(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))","execution_count":34,"outputs":[{"output_type":"stream","text":"PROFESSIONAL, EFFECTIVE DEBT COLLECTION SERVICES AVAILABLE\n\nFor the last seventeen years, National Credit Systems, Inc. has been providing\ntop flight debt collection services to over 15,000 businesses, institutions, and \nhealthcare providers.\n\nWe charge only a low-flat fee (less than $20) per account, and all proceeds are \nforwarded to you directly -- not to your collections agency.\n\nIf you wish, we will report unpaid accounts to Experian (formerly TRW), \nTRANSUNION, and Equifax. There is no charge for this important service.\n\nPLEASE LET US KNOW IF WE CAN BE OF SERVICE TO YOU.\n\nSimply reply to debt_collectors@chmailnet.com with the following instructions \nin the Subject field - \n\nREMOVE  --  Please remove me from your mailing list.\nEMAIL   --  Please email more information.\nFAX     --  Please fax more information.\nMAIL    --  Please snailmail more information.\nCALL    --  Please have a representative call.\n\nIndicate the best time to telephone and any necessary addresses and \ntelephone/fax numbers in the text of your reply.\n\nIf you prefer you can always telephone us during normal business hours \nat (212) 213-3000 Ext 1425.\n\nThank you.\n\nP.S. -- If you are not in need of our services at this time, please retain this \nmessage for future use or past it on to a friend. \n\n\n\nPrecision: 89.19%\nRecall: 95.85%\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}